import torch
from torch import nn

from base import BaseGanCriterion


# Adapted from https://github.com/meyerscetbon/LinearSinkhorn/blob/01943c72b03ea4b56df12ca5a9d0c1da2516d1ae/EXP_GAN/torch_lin_sinkhorn.py
class Lin_Sinkhorn_AD(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x_emb, y_emb, reg, niter_sin, lam=1e-6, tau=1e-9):
        phi_x = x_emb.squeeze().double().to(x_emb.device)
        phi_y = y_emb.squeeze().double().to(y_emb.device)

        n = phi_x.size()[0]
        m = phi_y.size()[0]

        a = (1.0 / n) * torch.ones(n)
        a = a.double().to(phi_x.device)

        b = (1.0 / m) * torch.ones(m)
        b = b.double().to(phi_y.device)

        actual_nits = 0

        u = 1.0 * torch.ones(n).double().to(phi_x.device)
        v = 1.0 * torch.ones(m).double().to(phi_y.device)
        err = 0.0

        u_trans = torch.matmul(phi_x, torch.matmul(phi_y.t(), v)) + lam
        v_trans = torch.matmul(phi_y, torch.matmul(phi_x.t(), u)) + lam

        for k in range(niter_sin):
            u = a / u_trans
            v_trans = torch.matmul(phi_y, torch.matmul(phi_x.t(), u)) + lam

            v = b / v_trans
            u_trans = torch.matmul(phi_x, torch.matmul(phi_y.t(), v)) + lam

            err = torch.sum(torch.abs(u * u_trans - a)) + torch.sum(
                torch.abs(v * v_trans - b)
            )

            actual_nits += 1
            if err < tau:
                break

            if k % 10 == 0:
                ### Stpping Criteria ###s
                with torch.no_grad():
                    err = torch.sum(torch.abs(u * u_trans - a)) + torch.sum(
                        torch.abs(v * v_trans - b)
                    )
                    if err < tau:
                        break

        ctx.u = u
        ctx.v = v
        ctx.reg = reg
        ctx.phi_x = phi_x
        ctx.phi_y = phi_y

        cost = reg * (torch.sum(a * torch.log(u)) + torch.sum(b * torch.log(v)) - 1)
        return cost

    @staticmethod
    def backward(ctx, grad_output):
        u = ctx.u
        v = ctx.v
        reg = ctx.reg
        phi_x = ctx.phi_x
        phi_y = ctx.phi_y

        grad_input = grad_output.clone()
        grad_phi_x = (
            grad_input
            * torch.matmul(u.view(-1, 1), torch.matmul(phi_y.t(), v).view(1, -1))
            * (-reg)
        )
        grad_phi_y = (
            grad_input
            * torch.matmul(v.view(-1, 1), torch.matmul(phi_x.t(), u).view(1, -1))
            * (-reg)
        )

        return grad_phi_x, grad_phi_y, None, None, None, None, None


class Critic(BaseGanCriterion):
    def __init__(self, parent, regularization=None, **kwargs):
        super().__init__(parent=parent, regularization=regularization, **kwargs)

        self.fn_loss = Lin_Sinkhorn_AD.apply

    def calculate(self, z, x, y):
        # Check D with real data
        critic_real = self.model["critic"](x)

        # Check D with fake data generated by G
        gen_x = self.model["generator"](z).detach()
        critic_fake = self.model["critic"](gen_x)

        # calculate criterion
        loss_real_fake = self.fn_loss(critic_real, critic_fake, self.configs['eps'], self.configs['n_iter'])
        loss_real_realv = self.fn_loss(critic_real, critic_real, self.configs['eps'], self.configs['n_iter'])
        loss_fake_fakev = self.fn_loss(critic_fake, critic_fake, self.configs['eps'], self.configs['n_iter'])

        loss = self.Tensor.MoneFloat.double() * (2 * loss_real_fake - loss_real_realv - loss_fake_fakev)

        # Return dict for regularization parameters
        reg_params = dict(
            network=self.model['critic'],
            real_data=x,
            fake_data=gen_x,
            critic_real=critic_real,
            critic_fake=critic_fake,
        )
        loss_scalar = {
            "critic_loss_xy": loss_real_fake,
            "critic_loss_xx": loss_real_realv,
            "critic_loss_yy": loss_real_realv
        }
        self.logger(Scalar=loss_scalar)
        return loss, reg_params


class Generator(BaseGanCriterion):
    def __init__(self, parent, regularization=None, **kwargs):
        super().__init__(parent=parent, regularization=regularization, **kwargs)
        self.fn_loss = Lin_Sinkhorn_AD.apply

    def calculate(self, z, x, y):
        # Check D with real data
        critic_real = self.model["critic"](x)

        # Check D with fake data generated by G
        gen_x = self.model["generator"](z)
        critic_fake = self.model["critic"](gen_x)

        # calculate criterion
        loss_real_fake = self.fn_loss(critic_real, critic_fake, self.configs['eps'], self.configs['n_iter'])
        loss_real_realv = self.fn_loss(critic_real, critic_real, self.configs['eps'], self.configs['n_iter'])
        loss_fake_fakev = self.fn_loss(critic_fake, critic_fake, self.configs['eps'], self.configs['n_iter'])

        loss = self.Tensor.OneFloat.double() * (2 * loss_real_fake - loss_real_realv - loss_fake_fakev)

        # Return dict for regularization parameters
        reg_params = dict(
            network=self.model['generator'],
            real_data=x,
            fake_data=gen_x,
            critic_real=critic_real,
            critic_fake=critic_fake,
        )
        loss_scalar = {
            "generator_loss_xy": loss_real_fake,
            "generator_loss_xx": loss_real_realv,
            "generator_loss_yy": loss_real_realv
        }
        self.logger(Scalar=loss_scalar)
        return loss, reg_params
