import numpy as np
import torch

from modules.trainer import UpdaterBase


class WGAN(UpdaterBase):

    def __init__(self, params, network, pre_train=None):
        super().__init__(params, network, pre_train)

        # Set optimizer
        for name in self.models.keys():
            self.models.set_optim(net=name, optimizer=torch.optim.RMSprop, lr=self.params["optimizer"]["lr_" + name])

        # Add fixed sample for testing
        self.fixed_samples = {
            "z_input": self.FloatTensor(
                np.random.normal(0, 1, (self.params["train"]["generated_size"], self.params["network"]["z_dim"])))
        }

    def update_parameters_discriminator(self, z, x, y):

        # Check D with real data
        d_real = self.models["Dis"](x)

        # Check D with fake data generated by G
        gen_x = self.models["Gen"](z).detach()
        d_fake = self.models["Dis"](gen_x)

        loss = torch.mean(d_fake) - torch.mean(d_real) + self.regularization(x, gen_x, d_real, d_fake)

        loss.backward()
        return loss

    def update_parameters_generator(self, z, x, y):

        # check G net
        gen_x = self.models["Gen"](z)
        d_fake = self.models["Dis"](gen_x)

        # calculate criterion
        loss = -torch.mean(d_fake)

        loss.backward()
        return loss

    def regularization(self, real_data, fake_data, d_real, d_fake):
        # Calculate Gradient_penalty
        if self.params["WGAN"]["gradient_penalty"]:
            gradient_penalty = trainer.regularization.regularization.gradient_penalty(self.models["Dis"], real_data,
                                                                                      fake_data,
                                                                                      self.params["WGAN"]["lambda_gp"])
        else:
            gradient_penalty = 0

        # calculate wassesrstein divergence
        if self.params["WGAN"]["w_div"]:
            w_div = trainer.regularization.regularization.wasserstein_div(real_data, fake_data, d_real, d_fake,
                                                                          self.params["WGAN"]["norm_power"],
                                                                          self.params["WGAN"]["div_coeff"])
        else:
            w_div = 0

        return gradient_penalty + w_div
