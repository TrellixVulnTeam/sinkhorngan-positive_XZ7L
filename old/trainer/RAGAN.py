import numpy as np
import torch

from modules.trainer import UpdaterBase


class RAGAN(UpdaterBase):

    def __init__(self, params, network, pre_train=None):
        super().__init__(params, network, pre_train)

        # Set optimizer
        for name in self.models.keys():
            self.models.set_optim(net=name, optimizer=torch.optim.Adam, lr=self.params["optimizer"]["lr_" + name],
                                  betas=self.params["optimizer"]["betas"])

        if torch.cuda.is_available():
            self.loss_func = torch.nn.BCEWithLogitsLoss().cuda()
        else:
            self.loss_func = torch.nn.BCEWithLogitsLoss()

        # Add fixed sample for testing
        self.fixed_samples = {
            "z_input": self.FloatTensor(
                np.random.normal(0, 1, (self.params["train"]["generated_size"], self.params["network"]["z_dim"])))
        }

    def update_parameters_discriminator(self, z, x, y):
        valid = self.FloatTensor(x.shape[0], 1).fill_(1.0)
        fake = self.FloatTensor(x.shape[0], 1).fill_(0.0)

        # Check D with real data
        d_real = self.models["Dis"](x)

        # Check D with fake data generated by G
        gen_x = self.models["Gen"](z.detach())
        d_fake = self.models["Dis"](gen_x)

        real_loss = self.loss_func(d_real - d_fake.mean(0, keepdim=True), valid)
        fake_loss = self.loss_func(d_fake - d_real.mean(0, keepdim=True), fake)

        loss = (real_loss + fake_loss) / 2

        loss.backward()
        return loss

    def update_parameters_generator(self, z, x, y):
        valid = self.FloatTensor(x.shape[0], 1).fill_(1.0)

        # Check D with real data
        d_real = self.models["Dis"](x).detach()

        # check G net
        gen_x = self.models["Gen"](z)
        d_fake = self.models["Dis"](gen_x)

        # calculate criterion
        loss = self.loss_func(d_fake - d_real.mean(0, keepdim=True), valid)
        loss.backward()
        return loss
